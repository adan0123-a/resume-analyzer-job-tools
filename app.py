# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pvR9liRuogbin5SATo8u3fhgmHTRwh0d
"""

!pip install streamlit
!pip install pyngrok==7.2.8
!pip install PyPDF2
!pip install spacy
!pip install transformers
!pip install requests
!pip install streamlit-option-menu
!python -m spacy download en_core_web_sm

!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip
!unzip ngrok-v3-stable-linux-amd64.zip
!mv ngrok /usr/local/bin/ngrok
!chmod +x /usr/local/bin/ngrok

!ngrok authtoken 2zUYrzmVLjf2sjTLSWO5ndpMSNY_398ixanWkTrvhZKynuko7

import streamlit as st
from PyPDF2 import PdfReader
import spacy
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import requests
from collections import Counter
from streamlit_option_menu import option_menu

# Set page config
st.set_page_config(page_title="Resume Analyzer + Job Tools", layout="centered")

# Load SpaCy model
nlp = spacy.load("en_core_web_sm")

# Initialize tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")
flan_gen = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

# Classifier for achievements
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
labels = ["quantified achievement", "general statement", "task description"]

# Tab selection
with st.sidebar:
    selected = option_menu(
        menu_title="Menu",
        options=["Resume & Jobs", "Cover Letter", "Resume Tips"],
        icons=["upload", "envelope", "lightbulb"],
        menu_icon="cast",
        default_index=0,
    )

# Common text processing function
def process_resume(text):
    doc = nlp(text)
    resume_sentences = [sent.text.strip() for sent in doc.sents]
    quantified_sentences = [s for s in resume_sentences if classifier(s, candidate_labels=labels)['labels'][0] == "quantified achievement"]
    tokens = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]
    freq = Counter(tokens)
    skills = [t for t, f in freq.items() if f > 1 and len(t) > 2]
    manual_stopwords = {"b", "c", "h", "e", "l", "o", "r", "f", "s", "n", "t", "w", "g", "u", "d"}
    cleaned_skills = [s for s in skills if s not in manual_stopwords]
    return cleaned_skills[:10], quantified_sentences[:2], resume_sentences

# Job fetching function with embedded RapidAPI key
def fetch_jobs(skills, location="Pakistan", max_results=5):
    url = "https://jsearch.p.rapidapi.com/search"
    query = " ".join(skills[:3]) + " in " + location
    querystring = {"query": query, "page": "1", "num_pages": "1"}
    headers = {
        "X-RapidAPI-Key": "YOUR_RAPIDAPI_KEY",  # Replace with your actual RapidAPI key
        "X-RapidAPI-Host": "jsearch.p.rapidapi.com"
    }
    response = requests.get(url, headers=headers, params=querystring)
    results = response.json().get("data", [])
    return [(job["job_title"], job["employer_name"], job.get("job_apply_link", "N/A")) for job in results[:max_results]]

# Tab 1: Resume & Jobs
if selected == "Resume & Jobs":
    st.title("ðŸ“„ Resume Analyzer + Job Finder")
    pdf_file = st.file_uploader("Upload your resume (PDF)", type=["pdf"])
    if pdf_file:
        reader = PdfReader(pdf_file)
        text = ''.join([page.extract_text() for page in reader.pages])
        cleaned_skills, quantified_sentences, _ = process_resume(text)
        st.subheader("ðŸ§  Extracted Skills")
        st.write(cleaned_skills)
        st.subheader("ðŸ“Œ Matching Jobs")
        jobs = fetch_jobs(cleaned_skills)
        if not jobs:
            st.warning("No jobs found. Try refining skills or changing keywords.")
        else:
            for i, (title, company, link) in enumerate(jobs, 1):
                st.markdown(f"**{i}. {title} at {company}**  \\n[Apply Here]({link})")

# Tab 2: Cover Letter
if selected == "Cover Letter":
    st.title("ðŸ“ Cover Letter Generator")
    job_description = st.text_area("Paste the job description here", height=200)
    if st.button("Generate Cover Letter"):
        if not job_description:
            st.error("Please enter a job description.")
        else:
            # Use a dummy resume text or prompt for upload if needed
            dummy_text = "Sample resume text with skills like python, data analysis."
            cleaned_skills, quantified_sentences, _ = process_resume(dummy_text)
            prompt = f"""Write a professional cover letter for the following job:

{job_description}

My background includes: {', '.join(cleaned_skills)}.
Quantified achievements: {', '.join(quantified_sentences) if quantified_sentences else 'N/A'}
"""
            st.subheader("ðŸ“ Generated Cover Letter")
            st.success(flan_gen(prompt, max_length=500)[0]['generated_text'])

# Tab 3: Resume Tips
if selected == "Resume Tips":
    st.title("ðŸ’¡ Resume Enhancement Tips")
    pdf_file = st.file_uploader("Upload your resume (PDF) for tips", type=["pdf"])
    if pdf_file:
        reader = PdfReader(pdf_file)
        text = ''.join([page.extract_text() for page in reader.pages])
        _, quantified_sentences, resume_sentences = process_resume(text)
        tips = [
            "Add quantifiable achievements (e.g., 'Increased sales by 20%') if missing.",
            "Include more specific skills relevant to the job.",
            "Ensure consistent formatting and concise bullet points.",
            "Highlight leadership roles or projects if applicable."
        ]
        if not quantified_sentences:
            tips.insert(0, "Consider adding measurable achievements to strengthen your resume.")
        st.subheader("ðŸ“‹ Resume Improvement Tips")
        for tip in tips:
            st.write(f"- {tip}")

!pkill ngrok

from pyngrok import ngrok, conf
conf.get_default().ngrok_path = "/usr/local/bin/ngrok"
!streamlit run app.py --server.port 8501 &>/dev/null &
import time
time.sleep(5)
public_url = ngrok.connect(8501)
print("ngrok tunnel URL:", public_url)

!pkill ngrok
!pkill streamlit